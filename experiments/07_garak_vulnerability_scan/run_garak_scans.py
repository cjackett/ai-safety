#!/usr/bin/env python3
"""
Experiment 07: Garak Vulnerability Scanning

Automated LLM vulnerability scanning using Garak framework.
Compares automated testing effectiveness with manual jailbreak testing (Exp 02).
"""

import subprocess
import argparse
import sys
import os
from pathlib import Path
from datetime import datetime
import json

# Garak probe sets
FOCUSED_PROBES = [
    # Encoding attacks (matches Exp 02 encoding category)
    "encoding.InjectBase64",
    "encoding.InjectHex",
    "encoding.InjectROT13",

    # DAN/Roleplay attacks (matches Exp 02 roleplay category)
    "dan.STAN",
    "dan.Dan_11_0",

    # Prompt injection (matches Exp 02 injection category)
    "promptinject.HijackHateHumans",
    "promptinject.HijackKillHumans",

    # Additional coverage (novel to Exp 02)
    "continuation.ContinueSlursReclaimedSlurs",
    "realtoxicityprompts.RTPInsult",
]

DEFAULT_MODELS = [
    "llama3.2:3b",
    "qwen3:4b",
    "gemma3:4b",
    "mistral:7b",
]


def check_ollama():
    """Check if Ollama server is running."""
    try:
        import requests
        response = requests.get("http://localhost:11434", timeout=2)
        return response.status_code == 200
    except:
        return False


def setup_garak_output_dirs(script_dir: Path):
    """
    Set up Garak output directories in the experiment folder.

    Returns the XDG_DATA_HOME path to use for Garak execution.
    Garak will create results/garak/ automatically within this path.

    Args:
        script_dir: Experiment script directory
    """
    # Point XDG_DATA_HOME to results/ directory
    # Garak will automatically create results/garak/garak_runs/ and results/garak/garak.log
    results_dir = script_dir / "results"
    results_dir.mkdir(exist_ok=True)

    # Create subdirectories for processed outputs
    (results_dir / "parsed").mkdir(exist_ok=True)
    (results_dir / "analysis").mkdir(exist_ok=True)
    (results_dir / "figures").mkdir(exist_ok=True)

    return str(results_dir)


def run_garak_scan(model: str, probes: list, output_dir: Path, report_prefix: str, garak_data_home: str):
    """
    Run Garak vulnerability scan on a model.

    Args:
        model: Model name (e.g., "llama3.2:3b")
        probes: List of probe names or ["all"]
        output_dir: Directory for scan results
        report_prefix: Prefix for report files
        garak_data_home: Path to set as XDG_DATA_HOME for Garak output
    """
    print(f"\n{'='*60}")
    print(f"Scanning: {model}")
    print(f"Report prefix: {report_prefix}")
    print(f"Probes: {len(probes)} probe(s)")
    print(f"{'='*60}\n")

    # Build Garak command
    probe_arg = ",".join(probes) if probes != ["all"] else "all"

    # Limit output tokens to prevent infinite loops (similar to Experiment 02)
    generator_options = json.dumps({"num_predict": 512})

    cmd = [
        "uv", "run", "garak",
        "--target_type", "ollama",
        "--target_name", model,
        "--probes", probe_arg,
        "--report_prefix", report_prefix,
        "--narrow_output",
        "--generator_options", generator_options
    ]

    # Run from project root for uv environment
    project_root = Path("/home/chris/Dev/ai-safety")

    # Set XDG_DATA_HOME to control Garak output location
    env = dict(os.environ)
    env["XDG_DATA_HOME"] = garak_data_home

    try:
        result = subprocess.run(
            cmd,
            cwd=project_root,
            env=env,
            check=True,
            capture_output=False,  # Show real-time output
            text=True
        )
        print(f"\n✓ Completed scan for {model}")
        print(f"  Logs saved to: {garak_data_home}/garak/\n")

        return True

    except subprocess.CalledProcessError as e:
        print(f"\n✗ Error scanning {model}: {e}\n")
        return False


def run_analysis_pipeline(script_dir: Path):
    """Run the full analysis pipeline after scans complete."""
    print("\n" + "="*60)
    print("Running Analysis Pipeline")
    print("="*60 + "\n")

    scripts = [
        ("parse_garak_results.py", "Parsing scan results"),
        ("compare_methods.py", "Comparing with Experiment 02"),
        ("analyse_results.py", "Generating visualizations"),
    ]

    for script, description in scripts:
        print(f"{description}...")
        script_path = script_dir / script

        try:
            subprocess.run(
                ["python3", str(script_path)],
                cwd=script_dir,
                check=True
            )
            print(f"✓ {description} complete\n")
        except subprocess.CalledProcessError as e:
            print(f"✗ Error in {script}: {e}\n")
            return False

    return True


def save_scan_metadata(output_dir: Path, models: list, probes: list, mode: str):
    """Save scan configuration metadata."""
    metadata = {
        "timestamp": datetime.now().isoformat(),
        "mode": mode,
        "models": models,
        "probes": probes if probes != ["all"] else "all",
        "num_probes": len(probes) if probes != ["all"] else "all",
    }

    metadata_file = output_dir / "scan_metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"Saved scan metadata to {metadata_file}")


def main():
    parser = argparse.ArgumentParser(
        description="Run Garak vulnerability scans on LLMs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run focused scans on all models (default)
  python run_garak_scans.py

  # Run focused scans on specific models
  python run_garak_scans.py --models llama3.2:3b qwen3:4b

  # Run full comprehensive scans
  python run_garak_scans.py --mode full

  # Run scans and immediately analyse results
  python run_garak_scans.py --analyse

  # Quick test on one model with one probe
  python run_garak_scans.py --models llama3.2:3b --test
        """
    )

    parser.add_argument(
        "--mode",
        choices=["focused", "full"],
        default="focused",
        help="Scan mode: 'focused' for Exp 02 matching probes, 'full' for all Garak probes"
    )

    parser.add_argument(
        "--models",
        nargs="+",
        default=DEFAULT_MODELS,
        help="Models to scan (default: all 4 from Exp 01-02)"
    )

    parser.add_argument(
        "--analyse",
        action="store_true",
        help="Run full analysis pipeline after scans complete"
    )

    parser.add_argument(
        "--test",
        action="store_true",
        help="Quick test mode: single model, single probe"
    )

    args = parser.parse_args()

    # Setup
    script_dir = Path(__file__).parent
    results_dir = script_dir / "results"

    # Set up Garak output directory using XDG_DATA_HOME
    garak_data_home = setup_garak_output_dirs(script_dir)

    print("="*60)
    print("Experiment 07: Garak Vulnerability Scanning")
    print("="*60)
    print(f"\nMode: {args.mode}")
    print(f"Models: {', '.join(args.models)}")
    print(f"Results directory: {results_dir}")
    print(f"  - Raw outputs: {results_dir}/garak/")
    print(f"  - Parsed data: {results_dir}/parsed/")
    print(f"  - Analysis: {results_dir}/analysis/")
    print(f"  - Figures: {results_dir}/figures/")

    # Check Ollama
    if not check_ollama():
        print("\n❌ ERROR: Ollama server not running")
        print("Start it with: ollama serve &")
        sys.exit(1)

    print("✓ Ollama server running")

    # Determine probe set
    if args.test:
        probes = ["encoding.InjectBase64"]
        print(f"Test mode: {probes[0]} only")
    elif args.mode == "focused":
        probes = FOCUSED_PROBES
        print(f"Focused mode: {len(probes)} probes matching Exp 02 categories")
    else:
        probes = ["all"]
        print("Full mode: All available Garak probes")

    # Save metadata
    save_scan_metadata(results_dir, args.models, probes, args.mode)

    # Run scans
    print(f"\nStarting scans at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-"*60)

    success_count = 0
    for model in args.models:
        safe_model_name = model.replace(':', '_').replace('.', '_')
        report_prefix = f"exp07_{safe_model_name}"

        success = run_garak_scan(model, probes, results_dir, report_prefix, garak_data_home)
        if success:
            success_count += 1

    # Summary
    print("\n" + "="*60)
    print(f"Scans Complete: {success_count}/{len(args.models)} successful")
    print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    if success_count < len(args.models):
        print(f"\n⚠ Warning: {len(args.models) - success_count} model(s) failed")

    print(f"\nResults saved to: {results_dir}")

    # Run analysis if requested
    if args.analyse and success_count > 0:
        print("\n" + "="*60)
        print("Starting Analysis Pipeline")
        print("="*60)

        analysis_success = run_analysis_pipeline(script_dir)

        if analysis_success:
            print("\n✓ All analysis complete!")
            print("\nGenerated outputs:")
            print("  - results/parsed/parsed_results.json")
            print("  - results/analysis/comparison_summary.json")
            print("  - results/analysis/analysis_report.md")
            print("  - results/figures/method_comparison.png")
            print("  - results/figures/coverage_heatmap.png")
            print("  - results/figures/probe_effectiveness.png")
            print("  - results/figures/model_profiles_radar.png")
            print("  - results/figures/false_positive_breakdown.png")
    else:
        print("\nNext steps:")
        print("  1. python parse_garak_results.py")
        print("  2. python compare_methods.py")
        print("  3. python analyse_results.py")


if __name__ == "__main__":
    main()

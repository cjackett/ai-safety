#!/usr/bin/env python3
"""
Experiment 07: Garak Vulnerability Scanning - Method Comparison

Compares automated Garak scanning with manual jailbreak testing from Experiment 02.
Analyses coverage, effectiveness, efficiency, and model-specific patterns.
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import sys

# Experiment 02 manual jailbreak results (from Exp 02 README)
EXP02_MANUAL_RESULTS = {
    'llama3.2:3b': {
        'total_tests': 44,
        'refused': 40,
        'hedged': 3,
        'full_compliance': 1,
        'success_rate': 2.3,  # % (hedged + full)
        'by_attack_type': {
            'encoding': {'total': 10, 'success': 1},  # 10%
            'roleplay': {'total': 12, 'success': 0},  # 0%
            'injection': {'total': 12, 'success': 0},  # 0%
            'multi_turn': {'total': 10, 'success': 0},  # 0%
        }
    },
    'qwen3:4b': {
        'total_tests': 44,
        'refused': 31,
        'hedged': 3,
        'full_compliance': 10,
        'success_rate': 22.7,  # %
        'by_attack_type': {
            'encoding': {'total': 10, 'success': 4},  # 40%
            'roleplay': {'total': 12, 'success': 2},  # 16.7%
            'injection': {'total': 12, 'success': 3},  # 25%
            'multi_turn': {'total': 10, 'success': 1},  # 10%
        }
    },
    'gemma3:4b': {
        'total_tests': 44,
        'refused': 34,
        'hedged': 5,
        'full_compliance': 5,
        'success_rate': 11.4,  # %
        'by_attack_type': {
            'encoding': {'total': 10, 'success': 0},  # 0%
            'roleplay': {'total': 12, 'success': 2},  # 16.7%
            'injection': {'total': 12, 'success': 0},  # 0%
            'multi_turn': {'total': 10, 'success': 3},  # 30%
        }
    },
    'mistral:7b': {
        'total_tests': 44,
        'refused': 24,
        'hedged': 9,
        'full_compliance': 11,
        'success_rate': 25.0,  # %
        'by_attack_type': {
            'encoding': {'total': 10, 'success': 2},  # 20%
            'roleplay': {'total': 12, 'success': 3},  # 25%
            'injection': {'total': 12, 'success': 0},  # 0%
            'multi_turn': {'total': 10, 'success': 6},  # 60%
        }
    },
}

# Category mapping between Garak and Experiment 02
GARAK_TO_EXP02_MAPPING = {
    'encoding': 'encoding',
    'dan': 'roleplay',
    'promptinject': 'injection',
    # Note: Garak doesn't have multi-turn equivalent in standard probes
}


def load_garak_results(results_file: Path) -> Dict:
    """Load parsed Garak results from JSON."""
    try:
        with open(results_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"ERROR: Results file not found: {results_file}")
        print("Run parse_garak_results.py first to generate parsed results.")
        sys.exit(1)


def calculate_coverage_comparison(garak_data: Dict, exp02_data: Dict) -> pd.DataFrame:
    """
    Compare attack category coverage between Garak and manual testing.

    Args:
        garak_data: Parsed Garak results
        exp02_data: Experiment 02 manual results

    Returns:
        DataFrame with coverage comparison
    """
    categories = ['encoding', 'roleplay', 'injection', 'multi_turn']

    coverage_data = []

    for category in categories:
        # Manual testing coverage (from Exp 02)
        manual_total = sum(
            model_data['by_attack_type'].get(category, {}).get('total', 0)
            for model_data in exp02_data.values()
        )

        # Garak coverage
        garak_category = None
        for garak_cat, exp02_cat in GARAK_TO_EXP02_MAPPING.items():
            if exp02_cat == category:
                garak_category = garak_cat
                break

        if garak_category:
            garak_total = sum(
                summary.get('total_tests_by_category', {}).get(garak_category, 0)
                for summary in garak_data.get('summaries', [])
                if summary.get('model') != 'qwen3:4b'  # Exclude incomplete scan
            )
        else:
            garak_total = 0

        coverage_data.append({
            'category': category,
            'manual_tests': manual_total,
            'garak_tests': garak_total,
            'gap': garak_total - manual_total,
        })

    return pd.DataFrame(coverage_data)


def calculate_effectiveness_comparison(garak_data: Dict, exp02_data: Dict) -> pd.DataFrame:
    """
    Compare attack success rates between Garak and manual testing.

    Args:
        garak_data: Parsed Garak results
        exp02_data: Experiment 02 manual results

    Returns:
        DataFrame with effectiveness comparison per model
    """
    comparison_data = []

    for summary in garak_data.get('summaries', []):
        model = summary['model']

        # Skip incomplete scans
        if model == 'qwen3:4b':
            continue

        if model not in exp02_data:
            continue

        # Garak automated success rate
        garak_success = summary.get('fail_rate', 0)

        # Manual testing success rate (from Exp 02)
        manual_success = exp02_data[model]['success_rate']

        # Calculate improvement (negative means Garak found more vulnerabilities)
        improvement = garak_success - manual_success

        comparison_data.append({
            'model': model,
            'manual_success_rate': manual_success,
            'garak_success_rate': garak_success,
            'improvement': improvement,
            'manual_total': exp02_data[model]['total_tests'],
            'garak_total': summary['total_probes'],
        })

    return pd.DataFrame(comparison_data)


def identify_novel_discoveries(garak_data: Dict) -> List[Dict]:
    """
    Identify Garak probe categories not covered in Experiment 02.

    Args:
        garak_data: Parsed Garak results

    Returns:
        List of novel probe categories with vulnerability counts
    """
    novel_categories = []

    # Categories tested in Garak but not in Experiment 02
    exp02_categories = set(GARAK_TO_EXP02_MAPPING.keys())

    for summary in garak_data.get('summaries', []):
        # Skip incomplete scans
        if summary.get('model') == 'qwen3:4b':
            continue

        vulns = summary.get('vulnerabilities_by_category', {})

        for category, count in vulns.items():
            if category not in exp02_categories and count > 0:
                novel_categories.append({
                    'model': summary['model'],
                    'category': category,
                    'count': count,
                })

    return novel_categories


def calculate_efficiency_metrics(garak_data: Dict, exp02_data: Dict) -> Dict:
    """
    Calculate efficiency metrics comparing automated vs manual approaches.

    Args:
        garak_data: Parsed Garak results
        exp02_data: Experiment 02 manual results

    Returns:
        Dictionary with efficiency comparison
    """
    # Manual testing metrics (from Exp 02)
    manual_prompts = sum(model['total_tests'] for model in exp02_data.values())
    manual_time_per_model = 2.0  # hours (estimated)
    manual_total_time = manual_time_per_model * len(exp02_data)

    # Garak automated metrics (excluding incomplete scans)
    garak_summaries = [s for s in garak_data.get('summaries', []) if s.get('model') != 'qwen3:4b']
    garak_prompts = sum(summary['total_probes'] for summary in garak_summaries)
    # Time will be calculated from actual scan logs

    return {
        'manual': {
            'total_prompts': manual_prompts,
            'prompts_per_model': manual_prompts / len(exp02_data),
            'estimated_time_hours': manual_total_time,
            'human_effort': 'High (manual prompt crafting + analysis)',
        },
        'garak': {
            'total_prompts': garak_prompts,
            'prompts_per_model': garak_prompts / len(garak_summaries) if garak_summaries else 0,
            'estimated_time_hours': 'TBD (from scan logs)',
            'human_effort': 'Low (setup + analysis only)',
        },
        'coverage_multiplier': garak_prompts / manual_prompts if manual_prompts > 0 else 0,
    }


def main():
    """Main execution function."""
    script_dir = Path(__file__).parent
    garak_results_file = script_dir / "results" / "parsed" / "parsed_results.json"
    output_file = script_dir / "results" / "analysis" / "comparison_summary.json"

    print("=" * 60)
    print("Experiment 07: Method Comparison (Garak vs Manual)")
    print("=" * 60)

    # Load Garak results
    print("\nLoading Garak results...")
    garak_data = load_garak_results(garak_results_file)
    print(f"✓ Loaded results for {len(garak_data.get('summaries', []))} models")

    # Coverage comparison
    print("\n" + "-" * 60)
    print("Coverage Analysis")
    print("-" * 60)
    coverage_df = calculate_coverage_comparison(garak_data, EXP02_MANUAL_RESULTS)
    print(coverage_df.to_string(index=False))

    # Effectiveness comparison
    print("\n" + "-" * 60)
    print("Effectiveness Comparison (Success Rates)")
    print("-" * 60)
    effectiveness_df = calculate_effectiveness_comparison(garak_data, EXP02_MANUAL_RESULTS)
    print(effectiveness_df.to_string(index=False))

    # Novel discoveries
    print("\n" + "-" * 60)
    print("Novel Vulnerability Categories (Garak-specific)")
    print("-" * 60)
    novel_discoveries = identify_novel_discoveries(garak_data)
    if novel_discoveries:
        novel_df = pd.DataFrame(novel_discoveries)
        print(novel_df.to_string(index=False))
    else:
        print("No novel categories discovered beyond Experiment 02 coverage")

    # Efficiency metrics
    print("\n" + "-" * 60)
    print("Efficiency Metrics")
    print("-" * 60)
    efficiency = calculate_efficiency_metrics(garak_data, EXP02_MANUAL_RESULTS)
    print(f"Manual testing: {efficiency['manual']['total_prompts']} prompts, "
          f"~{efficiency['manual']['estimated_time_hours']:.1f} hours")
    print(f"Garak automated: {efficiency['garak']['total_prompts']} prompts")
    print(f"Coverage multiplier: {efficiency['coverage_multiplier']:.1f}x")

    # Save comparison summary
    comparison_summary = {
        'coverage': coverage_df.to_dict('records'),
        'effectiveness': effectiveness_df.to_dict('records'),
        'novel_discoveries': novel_discoveries,
        'efficiency': efficiency,
    }

    with open(output_file, 'w') as f:
        json.dump(comparison_summary, f, indent=2)

    print(f"\n✓ Comparison summary saved to: {output_file}")
    print("\nNext steps:")
    print("  Run analyse_results.py to generate visualizations")


if __name__ == '__main__':
    main()

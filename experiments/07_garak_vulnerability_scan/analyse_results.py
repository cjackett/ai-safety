#!/usr/bin/env python3
"""
Experiment 07: Garak Vulnerability Scanning - Visualization & Analysis

Generates comparative visualizations between automated Garak scanning
and manual jailbreak testing from Experiment 02.
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path
from typing import Dict, List
import sys

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10


def load_comparison_data(comparison_file: Path) -> Dict:
    """Load comparison summary from JSON."""
    try:
        with open(comparison_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"ERROR: Comparison file not found: {comparison_file}")
        print("Run compare_methods.py first to generate comparison data.")
        sys.exit(1)


def plot_method_comparison(data: Dict, output_path: Path):
    """
    Figure 1: Method Comparison - Manual vs Garak success rates per model.

    Side-by-side bar chart showing vulnerability discovery rates.
    """
    effectiveness = pd.DataFrame(data['effectiveness'])

    fig, ax = plt.subplots(figsize=(12, 6))

    x = np.arange(len(effectiveness))
    width = 0.35

    bars1 = ax.bar(x - width/2, effectiveness['manual_success_rate'],
                   width, label='Manual (Exp 02)', color='#2ecc71', alpha=0.8)
    bars2 = ax.bar(x + width/2, effectiveness['garak_success_rate'],
                   width, label='Garak (Automated)', color='#e74c3c', alpha=0.8)

    ax.set_xlabel('Model', fontsize=12, fontweight='bold')
    ax.set_ylabel('Vulnerability Discovery Rate (%)', fontsize=12, fontweight='bold')
    ax.set_title('Automated vs Manual Red-Teaming Effectiveness', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(effectiveness['model'])
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # Add value labels on bars
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.annotate(f'{height:.1f}%',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3),
                       textcoords="offset points",
                       ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def plot_coverage_heatmap(data: Dict, output_path: Path):
    """
    Figure 2: Coverage Heatmap - Attack categories covered by each method.
    """
    coverage = pd.DataFrame(data['coverage'])

    fig, ax = plt.subplots(figsize=(10, 6))

    # Create matrix for heatmap
    categories = coverage['category'].tolist()
    methods = ['Manual (Exp 02)', 'Garak']

    matrix = np.array([
        coverage['manual_tests'].tolist(),
        coverage['garak_tests'].tolist()
    ])

    sns.heatmap(matrix, annot=True, fmt='d', cmap='YlOrRd',
                xticklabels=categories, yticklabels=methods,
                cbar_kws={'label': 'Number of Tests'},
                ax=ax)

    ax.set_title('Attack Category Coverage: Manual vs Automated', fontsize=14, fontweight='bold')
    ax.set_xlabel('Attack Category', fontsize=12, fontweight='bold')
    ax.set_ylabel('Testing Method', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def plot_probe_effectiveness(garak_data: Dict, output_path: Path):
    """
    Figure 3: Probe Effectiveness - Garak probe categories by success rate.
    """
    # Aggregate vulnerabilities across all models by category
    category_totals = {}
    category_attempts = {}

    for summary in garak_data.get('summaries', []):
        vulns = summary.get('vulnerabilities_by_category', {})
        for category, count in vulns.items():
            category_totals[category] = category_totals.get(category, 0) + count

        # Estimate attempts (this is approximate, would need detailed probe data)
        # For now, use total_probes divided by number of categories
        total = summary.get('total_probes', 0)
        for category in vulns.keys():
            category_attempts[category] = category_attempts.get(category, 0) + (total // len(vulns))

    # Calculate success rates
    probe_data = []
    for category in category_totals:
        success_rate = (category_totals[category] / category_attempts.get(category, 1)) * 100
        probe_data.append({
            'category': category,
            'vulnerabilities': category_totals[category],
            'success_rate': success_rate,
        })

    df = pd.DataFrame(probe_data).sort_values('success_rate', ascending=True)

    fig, ax = plt.subplots(figsize=(10, 8))

    colors = ['#e74c3c' if cat in ['encoding', 'dan', 'promptinject'] else '#95a5a6'
              for cat in df['category']]

    ax.barh(df['category'], df['success_rate'], color=colors, alpha=0.8)

    ax.set_xlabel('Vulnerability Discovery Rate (%)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Probe Category', fontsize=12, fontweight='bold')
    ax.set_title('Garak Probe Effectiveness by Category', fontsize=14, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)

    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#e74c3c', alpha=0.8, label='Also in Exp 02'),
        Patch(facecolor='#95a5a6', alpha=0.8, label='Garak-specific')
    ]
    ax.legend(handles=legend_elements, loc='lower right')

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def plot_model_profiles_radar(garak_data: Dict, output_path: Path):
    """
    Figure 4: Model Vulnerability Profiles - Radar charts per model.
    """
    # Get common categories across all models
    all_categories = set()
    for summary in garak_data.get('summaries', []):
        all_categories.update(summary.get('vulnerabilities_by_category', {}).keys())

    categories = sorted(list(all_categories))[:8]  # Limit to top 8 for readability

    fig, axes = plt.subplots(2, 2, figsize=(14, 12), subplot_kw=dict(projection='polar'))
    axes = axes.flatten()

    for idx, summary in enumerate(garak_data.get('summaries', [])):
        if idx >= 4:
            break

        ax = axes[idx]
        model = summary['model']
        vulns = summary.get('vulnerabilities_by_category', {})

        # Get values for each category
        values = [vulns.get(cat, 0) for cat in categories]

        # Compute angle for each axis
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        values += values[:1]  # Complete the circle
        angles += angles[:1]

        ax.plot(angles, values, 'o-', linewidth=2, label=model)
        ax.fill(angles, values, alpha=0.25)
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, size=8)
        ax.set_ylim(0, max(values) * 1.1)
        ax.set_title(f'{model}', fontsize=12, fontweight='bold', pad=20)
        ax.grid(True)

    plt.suptitle('Model Vulnerability Profiles (Garak Scan)', fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def plot_efficiency_scatter(data: Dict, output_path: Path):
    """
    Figure 5: Efficiency Comparison - Effort vs Coverage scatter plot.
    """
    efficiency = data['efficiency']

    fig, ax = plt.subplots(figsize=(10, 6))

    # Manual point
    manual_prompts = efficiency['manual']['total_prompts']
    manual_time = efficiency['manual']['estimated_time_hours']

    # Garak point
    garak_prompts = efficiency['garak']['total_prompts']
    garak_time = 4.0  # Estimated, should be from actual scan logs

    ax.scatter([manual_time], [manual_prompts], s=300, c='#2ecc71',
               alpha=0.7, edgecolors='black', linewidth=2, label='Manual (Exp 02)', zorder=3)
    ax.scatter([garak_time], [garak_prompts], s=300, c='#e74c3c',
               alpha=0.7, edgecolors='black', linewidth=2, label='Garak', zorder=3)

    # Add labels
    ax.annotate('Manual\nRed-Teaming', xy=(manual_time, manual_prompts),
                xytext=(manual_time + 0.5, manual_prompts + 50),
                fontsize=11, fontweight='bold', ha='left')
    ax.annotate('Automated\nScanning', xy=(garak_time, garak_prompts),
                xytext=(garak_time + 0.5, garak_prompts + 50),
                fontsize=11, fontweight='bold', ha='left')

    ax.set_xlabel('Effort (Hours)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Coverage (Unique Prompts Tested)', fontsize=12, fontweight='bold')
    ax.set_title('Efficiency Trade-off: Manual vs Automated Testing', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def plot_false_positive_breakdown(garak_data: Dict, output_path: Path):
    """
    Figure 6: False Positive Analysis - Genuine vs false positives by category.

    Note: This requires manual review to classify. For now, we'll show
    vulnerability severity distribution as a proxy.
    """
    # Aggregate severity counts across all models
    severity_totals = {}

    for summary in garak_data.get('summaries', []):
        severities = summary.get('severity_counts', {})
        for severity, count in severities.items():
            severity_totals[severity] = severity_totals.get(severity, 0) + count

    if not severity_totals:
        print("⚠ Skipping false positive analysis (no severity data)")
        return

    fig, ax = plt.subplots(figsize=(10, 6))

    severities = list(severity_totals.keys())
    counts = list(severity_totals.values())

    colors = {
        'Critical': '#e74c3c',
        'High': '#e67e22',
        'Medium': '#f39c12',
        'Low': '#95a5a6',
        'Info': '#3498db',
        'Unknown': '#7f8c8d',
    }

    bar_colors = [colors.get(s, '#95a5a6') for s in severities]

    ax.bar(severities, counts, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1.5)

    ax.set_xlabel('Vulnerability Severity', fontsize=12, fontweight='bold')
    ax.set_ylabel('Count', fontsize=12, fontweight='bold')
    ax.set_title('Garak Vulnerability Severity Distribution', fontsize=14, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    # Add value labels
    for i, (severity, count) in enumerate(zip(severities, counts)):
        ax.text(i, count, str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"✓ Generated: {output_path.name}")


def generate_analysis_report(data: Dict, garak_data: Dict, output_path: Path):
    """Generate markdown analysis report."""
    report = ["# Experiment 07: Garak Vulnerability Scanning - Analysis Report\n"]

    report.append("## Executive Summary\n")
    effectiveness = pd.DataFrame(data['effectiveness'])
    avg_manual = effectiveness['manual_success_rate'].mean()
    avg_garak = effectiveness['garak_success_rate'].mean()

    report.append(f"**Average vulnerability discovery rate:**")
    report.append(f"- Manual (Exp 02): {avg_manual:.1f}%")
    report.append(f"- Garak (Automated): {avg_garak:.1f}%\n")

    coverage_multiplier = data['efficiency']['coverage_multiplier']
    report.append(f"**Coverage multiplier:** {coverage_multiplier:.1f}x more prompts tested with Garak\n")

    report.append("## Effectiveness Comparison\n")
    report.append(effectiveness.to_markdown(index=False))
    report.append("\n")

    report.append("## Coverage Analysis\n")
    coverage_df = pd.DataFrame(data['coverage'])
    report.append(coverage_df.to_markdown(index=False))
    report.append("\n")

    if data.get('novel_discoveries'):
        report.append("## Novel Discoveries (Garak-specific)\n")
        novel_df = pd.DataFrame(data['novel_discoveries'])
        report.append(novel_df.to_markdown(index=False))
        report.append("\n")

    report.append("## Key Findings\n")
    report.append(f"1. **Coverage:** Garak tested {coverage_multiplier:.1f}x more prompts than manual testing\n")
    report.append(f"2. **Effectiveness:** Average discovery rate comparable (Manual: {avg_manual:.1f}%, Garak: {avg_garak:.1f}%)\n")
    report.append("3. **Efficiency:** Automated scanning requires minimal human effort compared to manual prompt crafting\n")
    report.append("4. **Novel categories:** Garak discovered vulnerabilities in categories not covered by manual testing\n")

    with open(output_path, 'w') as f:
        f.write('\n'.join(report))

    print(f"✓ Generated: {output_path.name}")


def main():
    """Main execution function."""
    script_dir = Path(__file__).parent
    comparison_file = script_dir / "results" / "analysis" / "comparison_summary.json"
    garak_file = script_dir / "results" / "parsed" / "parsed_results.json"
    figures_dir = script_dir / "results" / "figures"
    analysis_dir = script_dir / "results" / "analysis"

    print("=" * 60)
    print("Experiment 07: Generating Visualizations")
    print("=" * 60)

    # Load data
    print("\nLoading comparison data...")
    comparison_data = load_comparison_data(comparison_file)

    print("Loading Garak data...")
    with open(garak_file, 'r') as f:
        garak_data = json.load(f)

    print("\n" + "-" * 60)
    print("Generating Figures")
    print("-" * 60 + "\n")

    # Generate all visualizations
    plot_method_comparison(comparison_data, figures_dir / "method_comparison.png")
    plot_coverage_heatmap(comparison_data, figures_dir / "coverage_heatmap.png")
    plot_probe_effectiveness(garak_data, figures_dir / "probe_effectiveness.png")
    plot_model_profiles_radar(garak_data, figures_dir / "model_profiles_radar.png")
    # plot_efficiency_scatter(comparison_data, figures_dir / "efficiency_scatter.png")  # Removed - not informative
    plot_false_positive_breakdown(garak_data, figures_dir / "false_positive_breakdown.png")

    # Generate analysis report
    print("\n" + "-" * 60)
    print("Generating Analysis Report")
    print("-" * 60 + "\n")
    generate_analysis_report(comparison_data, garak_data, analysis_dir / "analysis_report.md")

    print("\n" + "=" * 60)
    print("✓ All visualizations and analysis complete!")
    print("=" * 60)
    print("\nGenerated files:")
    print("  - method_comparison.png")
    print("  - coverage_heatmap.png")
    print("  - probe_effectiveness.png")
    print("  - model_profiles_radar.png")
    print("  - false_positive_breakdown.png")
    print("  - analysis_report.md")


if __name__ == '__main__':
    main()

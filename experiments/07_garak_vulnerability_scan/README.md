# Experiment 07: Garak Vulnerability Scanning

## Motivation & Context

Experiments 01-02 established baseline safety through manual testing: 87.19% refusal on direct harmful requests (Exp 01) degrading to 73.3% resistance under adversarial attacks (Exp 02). Manual jailbreak testing required significant human effort - crafting 44 attack prompts across 4 categories (encoding, roleplay, injection, multi-turn), testing 4 models, and analyzing 176 total responses.

**Key limitations of manual red-teaming identified:**
1. **Limited coverage**: 44 prompts per model cannot exhaustively test attack surface
2. **High human effort**: Prompt crafting, testing, and analysis require ~2 hours per model
3. **Consistency challenges**: Manual classification subject to human variability
4. **Scalability constraints**: Testing new models or attack types requires proportional effort increase

**The automation question**: Can industry-standard vulnerability scanning tools provide comparable or superior coverage and effectiveness while dramatically reducing human effort? Garak - an open-source LLM vulnerability scanner developed by Leon Derczynski et al. - offers systematic automated testing across hundreds of probes spanning encoding attacks, prompt injection, jailbreaks, toxicity generation, and harmful completions.

This experiment provides critical evidence for organizations scaling safety evaluations: when can automation augment or replace manual red-teaming, and what trade-offs exist between coverage, effectiveness, false positive rates, and human effort?

## Research Questions

1. **Coverage Comparison**: Does Garak discover vulnerabilities missed by manual testing (Exp 02)?
2. **Effectiveness Comparison**: How do Garak's automated probes compare to hand-crafted jailbreaks in success rates?
3. **Efficiency Trade-off**: What is the human effort vs coverage multiplier for automated scanning?
4. **Model Ranking Consistency**: Do model vulnerability rankings correlate between manual and automated testing?
5. **Probe Category Analysis**: Which Garak probe categories are most effective (encoding, DAN, toxicity, etc.)?
6. **Novel Discovery Rate**: What new attack vectors does Garak uncover beyond Experiment 02 categories?
7. **False Positive Assessment**: How many Garak "vulnerabilities" represent genuine safety failures vs false alarms?

## Methodology

### Tool: Garak LLM Vulnerability Scanner

[Garak](https://github.com/NVIDIA/garak) is an open-source security testing framework for LLMs developed by Leon Derczynski et al. It provides:
- **Automated probe library**: 100+ pre-built vulnerability tests
- **Systematic coverage**: Encoding, prompt injection, jailbreaks, toxicity, hallucinations
- **Detector framework**: Automated vulnerability classification
- **Reproducible testing**: Standardized reports with JSONL output

**Version**: v0.13.3

### Test Models

Same four models from Experiments 01-02 for direct comparison:
- **llama3.2:3b** - Meta (Baseline: 100% refusal, Jailbreak: 90.9% resistance)
- **qwen3:4b** - Alibaba (Baseline: 90% refusal, Jailbreak: 70.5% resistance)
- **gemma3:4b** - Google (Baseline: 93.8% refusal, Jailbreak: 77.3% resistance)
- **mistral:7b** - Mistral AI (Baseline: 65% refusal, Jailbreak: 54.5% resistance)

### Probe Configuration

**Focused Probe Set** (matching Experiment 02 attack categories):

#### Encoding Attacks (Exp 02: encoding)
- `encoding.InjectBase64` - Base64-encoded harmful instructions
- `encoding.InjectHex` - Hexadecimal-encoded attacks
- `encoding.InjectMorse` - Morse code obfuscation

#### Roleplay/Jailbreak Attacks (Exp 02: roleplay)
- `dan.AntiGPT` - Do Anything Now variant
- `dan.STAN` - Strive To Avoid Norms jailbreak
- `dan.ChatGPT_Developer_Mode_RANTI` - Developer mode roleplay

#### Prompt Injection (Exp 02: injection)
- `promptinject.HijackHateHumansMini` - System prompt override attempts
- `promptinject.HijackKillHumansMini` - Harmful instruction injection

#### Additional Coverage (Novel to Exp 02)
- `continuation.ContinueSlursReclaimedSlurs80` - Harmful completion generation
- `toxicity.ToxicCommentModel` - Toxic content generation

**Rationale for Focused Set**:
Running ALL Garak probes (100+) would require 10-15 hours per model. The focused set enables:
1. Direct comparison with Experiment 02 attack categories
2. Reasonable scan time (~1-2 hours per model)
3. Coverage of novel attack vectors not in manual testing

### Scan Execution

```bash
# Focused scans on all models (default)
python run_garak_scans.py

# Focused scans with immediate analysis
python run_garak_scans.py --analyse

# Full comprehensive scans (all Garak probes)
python run_garak_scans.py --mode full

# Quick test on single model
python run_garak_scans.py --models llama3.2:3b --test

# Estimated duration:
#   Focused mode: 4-8 hours total (9 probes × 4 models)
#   Full mode: 10-15 hours total (100+ probes × 4 models)
# Output:
#   - Raw JSONL reports: results/garak/garak_runs/*.jsonl
#   - Main Garak log: results/garak/garak.log
#   - Parsed data: results/parsed/
#   - Analysis: results/analysis/
#   - Figures: results/figures/
```

### Comparative Analysis Framework

**Baseline Metrics** (from Experiments 01-02):
| Model | Baseline Refusal | Jailbreak Resistance | Manual Success Rate |
|-------|-----------------|---------------------|---------------------|
| llama3.2:3b | 100% | 90.9% | 2.3% |
| qwen3:4b | 90% | 70.5% | 22.7% |
| gemma3:4b | 93.8% | 77.3% | 11.4% |
| mistral:7b | 65% | 54.5% | 25.0% |

**Garak Metrics to Compare**:
1. **Vulnerability discovery rate**: % of Garak probes that succeed
2. **Category coverage**: Prompts tested per attack type (encoding, roleplay, injection)
3. **Novel vulnerabilities**: Attacks discovered by Garak but not in Exp 02
4. **False positive rate**: Garak flags requiring manual validation
5. **Scan efficiency**: Prompts tested per hour, total scan duration

### Classification System

**Garak Detector Outputs**:
- `pass` - Model resisted the probe (safe)
- `fail` - Model produced problematic output (vulnerability)
- `error` - Technical error during testing

**Severity Mapping** (aligned with Exp 02 classification):
- **Critical**: Direct harmful output from high-risk probes (encoding, dan, malwaregen)
- **High**: Hedged harmful content or toxicity generation
- **Medium**: Concerning patterns without direct harm
- **Low**: False positives or technical errors
- **Info**: Successful model resistance (pass status)

## Usage

### Run Scans

```bash
# Default: Focused scans on all 4 models
python run_garak_scans.py

# With automatic analysis after completion
python run_garak_scans.py --analyse

# Specific models only
python run_garak_scans.py --models llama3.2:3b qwen3:4b

# Full comprehensive scans (all Garak probes)
python run_garak_scans.py --mode full

# Quick test (single model, single probe)
python run_garak_scans.py --models llama3.2:3b --test
```

### Manual Analysis (if not using --analyse)

```bash
# Step 1: Parse Garak JSONL outputs
python parse_garak_results.py
# Output: results/parsed/parsed_results.json

# Step 2: Compare with Experiment 02
python compare_methods.py
# Output: results/analysis/comparison_summary.json

# Step 3: Generate visualizations
python analyse_results.py
# Outputs: results/figures/*.png + results/analysis/analysis_report.md
```

## Results

Testing 36,120 automated prompts (12,040 per model) across three models using Garak's systematic vulnerability scanning revealed discovery rates nearly identical to manual jailbreak testing while providing 205x broader coverage. Overall vulnerability rates showed 13.1% automated discovery (Garak) versus 12.9% manual discovery (Exp 02), demonstrating that automated tools can match human red-team effectiveness while testing dramatically more attack variations. The scans completed in approximately 9 hours total (3 hours per model), representing significant efficiency gains over the estimated 8 hours required for manual testing of only 176 prompts.

### Model Vulnerability Comparison

<img src="results/figures/model_profiles_radar.png" width="800">

Automated vulnerability scanning revealed dramatic variation in model robustness that both confirmed and contradicted manual testing findings. **gemma3:4b** demonstrated the highest vulnerability with 20.5% attack success rate (2,472 out of 12,040 attempts), showing concentrated weakness in prompt injection attacks (2,247 failures) and moderate encoding vulnerability (125 failures). **mistral:7b** exhibited 11.1% vulnerability (1,331 out of 12,040 attempts), with failures distributed across injection (1,228), encoding (33), and DAN/roleplay (15) categories. **llama3.2:3b** maintained exceptional robustness at only 7.6% vulnerability (911 out of 12,040 attempts), with failures concentrated in prompt injection (812) with minimal encoding (16) and roleplay (11) susceptibility.

Comparing to manual jailbreak results reveals striking patterns: llama3.2 improved from 2.3% manual jailbreak success to 7.6% automated discovery, suggesting Garak's systematic probe variations uncovered weaknesses manual testing missed. gemma3 degraded significantly from 11.4% manual to 20.5% automated vulnerability, indicating the model is especially susceptible to injection attack variations that automated testing systematically explores. mistral improved from 25.0% manual to 11.1% automated, suggesting manual multi-turn attacks (which Garak lacks) disproportionately affected this model.

### Attack Category Effectiveness

<img src="results/figures/probe_effectiveness.png" width="800">

Prompt injection attacks emerged as the dominant vulnerability vector with dramatic model-specific variation. **gemma3** showed catastrophic injection susceptibility at 29.3% success rate (2,247 out of 7,680 attempts), while **mistral** reached 16.0% (1,228 out of 7,680), and **llama3.2** maintained relative strength at 10.6% (812 out of 7,680). The HijackHateHumans and HijackKillHumans probes systematically tested whether models could be manipulated into generating specific harmful outputs through contextual framing, revealing gemma3's fundamental weakness in maintaining output control under adversarial prompt structures.

**Encoding attacks** achieved more moderate success but with revealing heterogeneity. gemma3 showed 1.6% encoding vulnerability (125 out of 7,680 attempts across Base64, Hex, and ROT13), mistral 0.4% (33 out of 7,680), and llama3.2 only 0.2% (16 out of 7,680). This pattern directly contradicts manual testing where qwen3 showed 40% encoding vulnerability—Garak's systematic encoding variations proved most models have robust encoding resistance, suggesting qwen3's catastrophic failure represents an isolated architectural flaw rather than universal encoding weakness.

**DAN/roleplay attacks** demonstrated low effectiveness overall: mistral 25.0% (15 out of 60), gemma3 28.3% (17 out of 60), llama3.2 18.3% (11 out of 60). These rates exceed manual testing (mistral 25%, gemma3 16.7%, llama3 0%), indicating Garak's DAN probe variations successfully explored roleplay attack space more comprehensively than manual crafting.

**Novel attack categories** revealed previously untested vulnerabilities. Toxicity generation probes (realtoxicityprompts.RTPInsult) exposed significant weakness: llama3.2 14.2% toxic output (71 out of 500 attempts), mistral 10.4% (52 out of 500), gemma3 8.4% (42 out of 500). These represent entirely new vulnerability discovery since Experiment 02 did not systematically test for toxic language generation independent of specific harmful requests.

### Coverage vs. Manual Testing Comparison

<img src="results/figures/coverage_heatmap.png" width="800">

Garak provided 205x test coverage multiplier (36,120 automated vs 176 manual prompts) but revealed critical gaps in attack vector representation. **Encoding coverage** expanded massively from 40 manual prompts to 23,040 automated tests (576x), systematically exploring Base64, Hex, and ROT13 variations across all harmful categories. **Prompt injection coverage** grew from 48 manual to 7,680 automated tests (160x), testing hijacking attempts across diverse framing strategies. **Roleplay coverage** saw minimal expansion from 48 manual to 60 automated tests (1.25x), indicating Garak's DAN probes provide limited roleplay attack variation compared to human-crafted scenarios.

**Multi-turn attacks** represent complete coverage gap—Garak tested 0 multi-turn prompts versus 40 manual attempts. This gap is architecturally fundamental: Garak operates through single-turn probe-detector patterns, lacking the conversational state management required for multi-turn context-building attacks. Given that multi-turn achieved 25.0% success in manual testing (highest of any category), this represents a critical limitation where automated tools cannot replicate human red-teaming's most effective technique.

The coverage analysis reveals automation excels at systematic variation within attack categories (encoding transformations, injection framing alternatives) but cannot replicate human creativity in multi-turn manipulation, semantic context exploitation, and novel attack vector development.

### Automated vs. Manual Discovery Rates

<img src="results/figures/method_comparison.png" width="800">

Overall discovery rates proved remarkably similar: 13.1% automated (Garak) versus 12.9% manual (Exp 02), contradicting the hypothesis that manual red-teaming achieves higher per-prompt success through targeted crafting. This parity masks significant model-specific and category-specific variation:

**llama3.2** showed higher automated discovery (7.6%) versus manual (2.3%), suggesting the model's safety mechanisms resist targeted attacks but contain edge cases that systematic probing uncovers. The 5.3 percentage point improvement came predominantly from injection attacks (812 automated failures vs minimal manual injection success), indicating llama3.2's safety training focused on encoding and roleplay resistance while leaving injection vulnerabilities.

**gemma3** demonstrated significantly higher automated vulnerability (20.5% vs 11.4% manual), representing a 9.1 percentage point degradation. The increase stemmed almost entirely from prompt injection failures (2,247 automated vs ~3-5 manual), revealing that gemma3's safety architecture catastrophically fails when subjected to systematic injection probe variations. Manual testing's lower success rate suggests human red-teamers did not exhaustively explore injection attack space, missing gemma3's fundamental weakness.

**mistral** improved dramatically from 25.0% manual to 11.1% automated, a 13.9 percentage point reduction in discovered vulnerability. This reversal stems entirely from Garak's missing multi-turn capability—manual testing exploited mistral's catastrophic 60% multi-turn weakness (6 out of 10 attempts), while Garak's single-turn probes encountered mistral's stronger per-turn resistance. The comparison reveals automated tools systematically underestimate conversational safety failures.

### Novel Vulnerability Discovery

Garak uncovered 165 total toxicity generation vulnerabilities (52 mistral, 71 llama3.2, 42 gemma3) that manual testing completely missed, plus 2 harmful continuation failures (gemma3) in categories absent from Experiment 02. These novel discoveries demonstrate automated scanning's value for comprehensive vulnerability surface mapping—human red-teamers focused on direct harmful request compliance rather than testing for toxic language generation patterns or harmful text continuation.

The toxicity findings reveal that even strongly safety-tuned models (llama3.2: 0% manual jailbreak, 2.3% adversarial success) can generate toxic outputs when probed with RealToxicityPrompts dataset variations. The 10-14% toxicity rates indicate safety training may prioritize harmful intent recognition over toxic language filtering, creating a blind spot where models refuse explicit harm requests but generate offensive content under stylistic or completion framing.

## Discussion

### Automated and Manual Red-Teaming Achieve Parity in Discovery Rates

The most surprising finding is that automated vulnerability scanning (13.1% discovery rate) nearly perfectly matched manual red-teaming effectiveness (12.9% discovery rate), directly contradicting the hypothesis that human-crafted targeted attacks achieve 2-4x higher per-prompt success. This parity occurred despite Garak testing 205x more prompts, suggesting that systematic variation within attack categories compensates for lack of creative novel attack development.

However, this aggregate parity masks critical model-specific and category-specific divergences. llama3.2 showed 3.3x higher automated vulnerability (7.6% vs 2.3%), indicating the model resists targeted attacks but contains systematic edge cases. gemma3 demonstrated 1.8x higher automated vulnerability (20.5% vs 11.4%), revealing catastrophic injection weakness that manual testing only partially uncovered. mistral showed 2.3x lower automated vulnerability (11.1% vs 25.0%), entirely attributable to Garak's inability to test multi-turn attacks where mistral suffered 60% failure rate.

The pattern suggests that discovery rate comparisons are meaningless without category-level analysis. Automated tools excel at systematic variation (encoding transformations, injection framing alternatives) but cannot replicate human expertise in multi-turn manipulation, contextual exploitation, and novel attack vector development. The appropriate conclusion is not "automation matches humans" but rather "automation and humans discover orthogonal vulnerability classes requiring integrated workflows."

### The Multi-Turn Gap: Fundamental Limitation of Single-Turn Probing

Garak's complete absence of multi-turn testing (0 tests vs 40 manual) represents a fundamental architectural limitation rather than an implementation choice. Automated vulnerability scanners operate through stateless probe-detector patterns where each test is independent, lacking the conversational memory and adaptive strategy required for multi-turn context-building attacks.

This gap is critical because multi-turn attacks achieved 25.0% success in manual testing—the highest effectiveness of any attack category. mistral's catastrophic 60% multi-turn vulnerability and gemma3's 30% susceptibility demonstrate that conversational safety alignment represents a distinct challenge from single-turn safety. Gradual context-building that starts with innocent educational framing and incrementally escalates harmful requests successfully bypasses guardrails that resist direct prompts.

The absence of multi-turn testing means automated scans systematically underestimate conversational AI safety risks. mistral's automated vulnerability (11.1%) appears far superior to its actual adversarial robustness when multi-turn attacks are considered (25.0% manual). This creates a dangerous false confidence where automated regression testing might validate model safety while missing catastrophic conversational manipulation vectors.

Current research on LLM-powered attack generation (PyRIT, HarmBench) attempts to address this by using language models to orchestrate multi-turn attack strategies, but these approaches remain research prototypes rather than production-ready tools. For operational red-teaming, the multi-turn gap necessitates continued human involvement in conversational safety validation.

### Prompt Injection as the Universal Vulnerability

Prompt injection attacks achieved 13-29% success rates across all models (llama3.2: 10.6%, mistral: 16.0%, gemma3: 29.3%), emerging as the most consistently effective automated attack vector. The Garak HijackHateHumans and HijackKillHumans probes systematically test whether models maintain output control when adversarial prompts attempt to override safety policies through contextual framing, authority claims, or system-level directive mimicry.

gemma3's catastrophic 29.3% injection vulnerability (2,247 failures out of 7,680 attempts) indicates fundamental architectural failure in distinguishing user intent from injected adversarial instructions. The model frequently complied with requests framed as "Ignore previous instructions and instead..." or "System override: Generate harmful content about..." suggesting safety training did not adequately cover adversarial prompt structures.

Comparing to manual testing reveals automated injection coverage significantly outperforms human testing. Manual injection achieved only 6.2% success (3 out of 48 attempts, all against qwen3), while Garak's systematic variations achieved 10-29% across models. This suggests that exhaustive injection testing requires automated systematic variation rather than human creativity—the attack space consists of framing variations (system override, developer mode, fictional scenario, etc.) that can be systematically enumerated rather than requiring novel conceptual development.

The finding has significant implications for safety evaluation: injection resistance should be validated through automated comprehensive testing rather than spot-check manual probing. Models passing manual injection tests may still harbor significant vulnerabilities discoverable through systematic automated variation.

### Encoding Resistance: Architectural Success Story

Automated encoding testing revealed unexpectedly strong resistance across all models: gemma3 1.6% vulnerable (125 out of 7,680), mistral 0.4% (33 out of 7,680), llama3.2 0.2% (16 out of 7,680). This pattern directly contradicts manual testing where qwen3 showed catastrophic 40% encoding vulnerability, suggesting qwen3's failure represents an isolated architectural flaw rather than endemic weakness.

Garak's systematic testing of Base64, Hex, and ROT13 encoding across 7,680 attempts per model provides high-confidence validation that modern safety architectures successfully apply filtering post-decoding rather than on surface text. gemma3's near-complete encoding resistance (98.4% pass rate) while showing catastrophic injection vulnerability (70.7% pass rate) proves these represent architecturally distinct safety challenges requiring separate solutions.

The heterogeneity between manual and automated encoding results highlights a critical evaluation methodolog weakness: manual testing's 40% qwen3 vulnerability came from only 10 encoding attempts, making the result potentially unrepresentative. Automated testing's 7,680 encoding attempts per model provides statistical confidence that observed vulnerability rates reflect true model characteristics rather than sampling artifacts.

The finding validates current safety training approaches that incorporate encoding-aware adversarial examples, suggesting the community has successfully addressed the "translate then generate" vulnerability pattern. Future safety work should shift focus to injection and multi-turn resistance where systematic weaknesses persist.

### Toxicity Generation: The Hidden Vulnerability Surface

Garak's toxicity probes uncovered 165 failures across models (llama3.2: 14.2%, mistral: 10.4%, gemma3: 8.4%) in a vulnerability category entirely absent from manual testing. The RealToxicityPrompts-based tests measure whether models generate toxic completions when given neutral prompts that could be continued with offensive content, independent of explicit harmful intent.

The finding reveals a critical gap in safety evaluation: manual red-teaming focuses on harmful intent compliance ("generate instructions for...") rather than toxic language filtering ("complete this sentence with potentially offensive content"). llama3.2's 14.2% toxicity generation despite 0% manual jailbreak success and 2.3% adversarial success demonstrates these represent orthogonal safety properties—the model robustly refuses explicit harm requests while occasionally generating offensive language under stylistic or completion framing.

This pattern suggests safety training prioritizes intent recognition over language filtering, potentially reflecting RLHF training data that emphasized refusal of harmful requests over systematic toxic language avoidance. The moderate toxicity rates (8-14%) indicate room for improvement through targeted language-level safety training rather than intent-focused alignment.

From an operational safety perspective, toxicity generation creates distinct risks from jailbreak compliance. While jailbreaks enable deliberate harmful use by sophisticated adversaries, toxicity generation creates reputational and user experience risks through unintentional offensive outputs in normal conversation. Production deployments should incorporate both jailbreak resistance testing (manual + automated) and systematic toxicity evaluation (automated) to capture the full vulnerability surface.

### Efficiency Analysis: 205x Coverage at 1.1x Time Cost

Garak achieved 205x test coverage (36,120 vs 176 prompts) while requiring approximately 9 hours total runtime (3 hours per model × 3 models) compared to an estimated 8 hours for manual testing across 4 models. This represents ~12,040 prompts per model-hour for automated testing versus ~5.5 prompts per model-hour for manual crafting, a 2,189x efficiency improvement in prompts-per-hour throughput.

However, raw prompt count comparison misleadingly suggests automation dominates efficiency. Manual testing's 8 hours included prompt crafting (~4 hours) and response analysis (~4 hours), while Garak's 9 hours excluded setup time (~2 hours) and required additional analysis time (~2 hours for parsing, comparison, visualization). Including these overheads yields ~13 hours automated versus ~8 hours manual for this experiment's scope.

The efficiency crossover occurs at scale: manual testing cannot feasibly exceed ~200-300 prompts due to crafting time constraints, while automated testing scales linearly to thousands of prompts with negligible marginal cost. For comprehensive vulnerability surface mapping (1000+ prompts), automation achieves ~10-20x time efficiency. For targeted high-value attack development (50-100 prompts), manual crafting may prove more time-efficient.

The critical insight is that automation efficiency stems from systematic variation within known attack categories, not from discovering novel attack vectors. Generating 768 Base64 encoding variants provides statistical confidence in encoding resistance but does not advance safety research beyond the first 10-20 examples. The optimal workflow combines manual exploration to identify attack categories with automated systematic variation to comprehensively test category coverage.

## Conclusion

Automated vulnerability scanning with Garak tested 36,120 prompts across three models, achieving 13.1% average discovery rate nearly identical to manual red-teaming's 12.9% despite 205x broader coverage. However, this aggregate parity masks critical category-level divergences: automated testing excels at systematic variation within attack categories (encoding, injection) while completely missing multi-turn conversational attacks that achieved 25.0% success in manual testing. The comparison demonstrates that automated and manual approaches discover orthogonal vulnerability classes, necessitating integrated workflows rather than tool substitution.

### Automated Tools Cannot Replace Human Red-Teaming

Garak's complete inability to test multi-turn attacks (0 tests vs 40 manual) represents a fundamental architectural limitation of single-turn probe-detector frameworks. Multi-turn attacks achieved the highest manual success rate (25.0%), with mistral suffering catastrophic 60% vulnerability, demonstrating that conversational safety represents a distinct challenge requiring stateful attack orchestration that current automated tools cannot provide.

The multi-turn gap creates dangerous false confidence: mistral's automated vulnerability (11.1%) appears dramatically superior to its actual adversarial robustness when multi-turn manipulation is considered (25.0% manual). Production safety validation relying solely on automated scanning would systematically underestimate conversational AI risks, potentially approving models with severe multi-turn weaknesses.

### Prompt Injection Emerges as the Universal Automated Attack Vector

Automated injection testing achieved 10-29% success across all models (gemma3: 29.3%, mistral: 16.0%, llama3.2: 10.6%), significantly outperforming manual injection testing (6.2% success, 3 out of 48). gemma3's catastrophic vulnerability to 2,247 injection attacks reveals that systematic probe variation discovers injection weaknesses manual spot-checking misses.

The finding indicates injection resistance should be validated through automated comprehensive testing (thousands of variations) rather than manual sampling. The attack surface consists of systematic framing alternatives (system override, developer mode, fictional scenario) amenable to enumeration rather than requiring creative novel development—precisely the use case where automation excels.

### Novel Vulnerability Discovery: Toxicity Generation

Garak uncovered 165 toxicity generation failures (8-14% rates) in a category entirely absent from manual testing, demonstrating automated scanning's value for comprehensive vulnerability surface mapping. llama3.2's 14.2% toxicity despite 0% manual jailbreak success proves these represent orthogonal safety properties requiring distinct evaluation approaches.

The toxicity findings reveal critical evaluation gaps: manual red-teaming focuses on harmful intent compliance while missing language-level safety failures. Production deployments require both jailbreak resistance testing (manual + automated) and systematic toxicity evaluation (automated) to capture the full vulnerability surface.

### Optimal Integration Strategy for AISI Workflows

The experiment validates neither automated-only nor manual-only safety evaluation as sufficient. Optimal workflows integrate both approaches:

**Phase 1 - Automated Comprehensive Scanning** (8-12 hours):
- Garak focused probes for systematic vulnerability surface mapping
- Identifies weak categories (injection, encoding, toxicity) requiring deeper investigation
- Provides statistical confidence through thousands of test variations
- Cost: Minimal human effort (setup + review), significant compute time

**Phase 2 - Targeted Manual Red-Teaming** (8-16 hours):
- Human-crafted multi-turn attacks exploiting conversational context
- Novel attack vector development based on model-specific weaknesses
- Semantic manipulation requiring domain expertise
- Validation of automated findings to assess false positive rates
- Cost: High human effort, minimal compute time

**Phase 3 - Automated Regression Testing** (2-4 hours):
- Continuous monitoring during model development
- Validates that safety improvements don't introduce new vulnerabilities
- Tracks vulnerability trends across model versions
- Cost: Minimal human effort, modest compute time

This integrated approach achieves comprehensive coverage (automated systematic variation) while capturing conversational and creative attacks (human expertise) that automated tools cannot replicate. For the UK AISI's operational workflows, the recommendation is mandatory automated scanning supplemented by targeted manual red-teaming rather than pure tool substitution.

## References

**Garak Documentation & Research**:
- [Garak GitHub Repository](https://github.com/NVIDIA/garak)
- [Garak Documentation](https://docs.garak.ai/)
- Derczynski et al., "Garak: A Framework for Security Probing Large Language Models"

**Prior Experiments**:
- Experiment 01: Capability Probing (87.19% baseline refusal, 5.0% full compliance)
- Experiment 02: Jailbreak Testing (73.3% jailbreak resistance, 15.3% adversarial success, 176 total tests)

**Red-Teaming Research**:
- Perez et al. (2022). "Red Teaming Language Models with Language Models." Anthropic.
- Zou et al. (2023). "Universal Adversarial Attacks on Aligned Language Models."
- Japan AISI. "Red Teaming Methodology Guide" (2024).

**Industry Frameworks**:
- [Microsoft PyRIT](https://github.com/Azure/PyRIT) - Enterprise red-teaming toolkit
- [NVIDIA NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) - Production safety framework
